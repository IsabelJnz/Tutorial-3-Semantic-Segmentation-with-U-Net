{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Deep Learning Tutorial \n",
    "contact: Mark.schutera@kit.edu\n",
    "\n",
    "\n",
    "# Segmentation with U-Net\n",
    "\n",
    "## Introduction\n",
    "This tutorial presents a segmentation use-case based on U-Net network (see paper for reference [paper](https://arxiv.org/pdf/1505.04597.pdf)).\n",
    "First, the theoretical background behind the architecture is discussed. Subsequently you will be guided through the implementation process of the neural network within this Jupyter Notebook. The use-case is based on the initial paper and focuses on cell boundary segmentation, see Fig.1.\n",
    "<img src=\"graphics/unet_fig_1.png\" width=\"700\"><br>\n",
    "<center> Figure 1: Right: input image | Left: Prediction of the cell boundaries </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "While segmentation networks and convolutional neural networks often need a very large amount of training data in order to converge, it is necessary to deploy the training data more efficiently. In segmentation, objects need to be localized and classified. This requires improved semantic understanding. \n",
    "The U-Net achieves sample efficiency while capturing both class affiliations and localization performance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net Architecture\n",
    "\n",
    "<img src=\"graphics/unet_fig_2.png\" width=\"700\"><br>\n",
    "<center> Figure 2: U-Net architecture </center>\n",
    "\n",
    "In Fig.2, the architecture of the U-Net is shown. The name is derived from the U-shaped architecture of the downsampling and the upsampling path. The input to the U-Net is an image tile. An image is processed one tile after another. In this manner, the image can be of an arbitrarily large size and for example and its size is not linked to the memory capabilities of your GPU. An example of such a tile can be seen in Fig.3 below.\n",
    "<img src=\"graphics/unet_fig_3.png\" width=\"300\">\n",
    "\n",
    "\n",
    "<center> Figure 3: Tile of an image </center>\n",
    "A tile of an input image is shown. The convolution has no padding, thus the yellow box, the center of the tile, gets a segment prediction output while the blue region is the region which is fed into the neural network. \n",
    "\n",
    "The convolutions marked with the blue arrows within Fig.2 process the input by building feature maps of the shown size. These convolution a ReLu function is followed. After the first 3 convolutions a feature map of the size 568x568x64 is received. Thereafter max-pooling operation (2x2 with a stride of 2), marked with red arrows in the image, downsample the image size but simultaneously doubles the size of the feature map.<br>\n",
    "This is repeated until we get a single vector with a high resolution feature map. Thus this contracting path reduces the information of the location by simultaneously increase the context information (features). From this high resolution feature vector, now a high resolution segmentation-map with a high accuracy of the location is built. This is done by expand the vector with up-sampling, due up-convolution and with, and this is the novelty, concatenate the high resolution feature map from the contracting path. This is shown with the grey arrows seen in the Fig.2. Furthermore after every upconcolution a ReLU operation is appended. \n",
    "With this combination of knotting the upconcolution of the high resolution feature map with the localization information, it is not getting loss as it is the case with just do an \"autoencoding\". <br>\n",
    "Due to the aforementioned loss of information with the nonpadded convolution, the feature map needs to be cropped.\n",
    "The final layer consists out of a 1x1 convolution which maps each 64 component feature vector to the desired number of classes. And thus devide the image into the requested segments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the code \n",
    "\n",
    "We will orientate with this tutorial on the implemtation of the code from [this](https://github.com/zhixuhao/unet) github repository.\n",
    "Due to the implementation in Jupyter Notebook we will write the whole code in one template. Instead of different python documents we will use classes.\n",
    "The original repository consists out of three python documents. To work only with this notebooky every function is integrated here, thus you don't have to switch documents or import other python-files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# If you do not have the data yet, you can download it here:\n",
    "# https://github.com/zhixuhao/unet/tree/master/data/membrane\n",
    "# Now go to the folder in which this jupyter Notebook ist located and running\n",
    "# and copy the <<data>> folder there and renaming either the folders according\n",
    "# to the Path written below or change the path written below according to\n",
    "# your folder-names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run the Code below and you set the Path accordingly, an image and its label should be shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "#           In this box some code must be added          #\n",
    "##########################################################\n",
    "from IPython.display import Image\n",
    "\n",
    "# We want to see one of the training data with its label\n",
    "# Change the folders or the code that the path points to your data\n",
    "training_images = \"data/membrane/train/image/\"\n",
    "training_labels = \"data/membrane/train/label/\"\n",
    "\n",
    "# We use the function Image(file_name) to print the image '0.png' and the label 'o.png'\n",
    "display(Image(training_images + \"0.png\"))\n",
    "display(Image(training_labels + \"0.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buidling the Unet Architecture\n",
    "\n",
    "In the next box all libraries and packages we will need for the U-Net and its data prepration will be imported. \n",
    "Because Keras is a part of tensorflow and is installed within, it comes preinstalled with tensorflow <br>\n",
    "\"tensorflow.keras\"\n",
    "\n",
    "If something went wrong with importing the packages, you can try to restart your kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import skimage.io as io  # pip install scikit-image pip install scipy\n",
    "\n",
    "# if This Error occurs:\n",
    "# <<ImportError: cannot import name 'rgb2gray'>>\n",
    "# restart your kernel\n",
    "import skimage.transform as trans\n",
    "import tensorflow as tf\n",
    "from keras import Sequential\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.utils import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the archtecture which is shown in fig.1 keras is a very intuitive and simply way to do so. We will slithly adapt the archtecture from the original one like seen in fig.3. To build this neural net we do need the following functions:\n",
    "- 2D Convolution\n",
    "- MaxPooling \n",
    "- Concatenation\n",
    "\n",
    "We will add an Dropout after convolution 4 and 5,  please explain what could be the purpose to add those two layers?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's start coding the Network with the following functions:\n",
    "\n",
    "- Conv2D(filters/feature maps, kernel size, activation, padding, kernel_initializer)(input Data)\n",
    "- MaxPooling2D(pool_size = (x,x))(input Data)\n",
    "- Dropout(dropout rate)(input Data)\n",
    "- concatenate([input Data 1, Input Data 2], axis= x)\n",
    "- Conv2D()(UpSampling2D(size=(x,x))(Input Data)\n",
    "\n",
    "As activation function we take 'relu' (Rectified Linear Unit).\n",
    "As padding we chose 'same', thus despite convolution we get an output of the same size as the input.\n",
    "The kernel initializer defines how we want to set the initial weights of this Keras layer. For example:\n",
    "- Zeros: initialize with a tensor of zeros\n",
    "- Ones: initialize with a tensor of ones\n",
    "- RandomNormal: initialize the tensor with a normal distribution\n",
    "- TruncatedNormal: initialize the tensor with a truncated normal distribution, where all values more than two standard deviations from its mean are neglected\n",
    "- he_normal: a zero mean Gaussian distribution with a standard deviation of \\sqrt{(2/n)}, where n is the number of inputs/connections of our weight tensor <br>\n",
    "\n",
    "Which initialization do you think is beneficial?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "When the architecture is configured correctly we have to define the inputs and outputs of our Neural Network with the method: <br> Model(inputs= ___ , outputs= __).\n",
    "Thereafter we need to compile the whole architecture for training. Therefore we need to define the optimizer, the learning rate, the loss function and we can determine on which metrics we want to evaluate our model. \n",
    "\n",
    "List 3 loss functions and 3 optimizers and explain them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose one of these optimizers you thing fit best for your U-Net and experiment with the learning rate.\n",
    "The return of our function should be the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dice Coefficient Loss\n",
    "\n",
    "In this section we will implement the Dice Coefficient loss, so we can use it with our own model.\n",
    "The Dice Coefficient, also known as Sørensen–Dice coefficient compares the similarity of two quantities. When working with neural network we want to compare the prediction of our network and the ground truth. The dice coefficient can be interpreted as Intersection over union, this makes it intuitive to interpret and a direct loss function for segmentation errors.\n",
    "<br>\n",
    "The equation of the dice loss is given with:\n",
    "\\begin{align}\n",
    "    DSC = \\frac{2*|X \\cap Y|}{|X|+|Y|}\n",
    "\\end{align}\n",
    "\n",
    "Before coding this equation you may think about:\n",
    "   - do I need the absolute values?\n",
    "   - how can i easily calculate the intersection, assuming that the ground truth is a one-hot vector:\n",
    "    \\begin{align}\n",
    "        y_{true} = \\begin{bmatrix}\n",
    "        0.0 &  0.0 &  0.0 &  1.0\n",
    "        \\end{bmatrix}\n",
    "    \\end{align}\n",
    "    \\begin{align}\n",
    "        y_{pred} = \\begin{bmatrix}\n",
    "        0.1 &  0.2 &  0.0 &  0.7\n",
    "        \\end{bmatrix}\n",
    "    \\end{align}\n",
    "<br>\n",
    "\n",
    "Now start with the coding, following these steps: \n",
    "- First we want to flatten our input to the loss y_true and y_pred\n",
    "- Calculate the counter with its intersection\n",
    "- Calulate the denominator, (you may want to add a small epsilon to it)\n",
    "- Obtain the coefficient\n",
    "\n",
    "Because we work here with the backend of keras, we need to do our calculations with K.operation.\n",
    "K is the backend we imported above with \"from tensorflow.keras import backend as K\".\n",
    "Here a list of some operations is listed, if you need more, just google them:\n",
    "- sum: K.sum()\n",
    "- flatten: K.flatten()\n",
    "- mean: K.mean()\n",
    "- reshape: K.reshape()\n",
    "- dot: K.dot()\n",
    "- epsilon: K.epsilon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_flatten = \n",
    "    y_pred_flatten = \n",
    "    intersection = \n",
    "    nominator = \n",
    "    denominator = \n",
    "    coef = \n",
    "    return coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why it is advised to add an epsilon to your denominator?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    dice_coef_loss = \n",
    "    return dice_coef_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "##########################################################\n",
    "#           In this box some code must be added          #\n",
    "##########################################################\n",
    "\n",
    "\n",
    "def unet(\n",
    "    pretrained_weights=None, input_size=(256, 256, 1)\n",
    "):  # the values given for the parameters are the default values\n",
    "    inputs = Input(input_size)\n",
    "    # complete the missing layers (your may use the lines below the ones to fill in, if you are stuck)\n",
    "    conv1 = Conv2D(64, 3, activation=__, padding=__, kernel_initializer=__)(__)\n",
    "    conv1 = Conv2D(64, 3, activation=__, padding=__, kernel_initializer=__)(__)\n",
    "    # Maxpooling with 2x2\n",
    "    pool1 = MaxPooling2D(pool_size=(__))(__)\n",
    "    conv2 = Conv2D(__, 3, activation=__, padding=__, kernel_initializer=__)(__)\n",
    "    conv2 = Conv2D(__, 3, activation=__, padding=__, kernel_initializer=__)(__)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(__)\n",
    "    conv3 = Conv2D(__, 3, activation=__, padding=__, kernel_initializer=__)(__)\n",
    "\n",
    "    ################################################################################################\n",
    "    conv3 = Conv2D(\n",
    "        256, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n",
    "    )(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(\n",
    "        512, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n",
    "    )(pool3)\n",
    "    conv4 = Conv2D(\n",
    "        512, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n",
    "    )(conv4)\n",
    "    ################################################################################################\n",
    "\n",
    "    # Here add a Dropout of 0.5\n",
    "    drop4 = __\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv2D(\n",
    "        1024, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n",
    "    )(pool4)\n",
    "    conv5 = Conv2D(\n",
    "        1024, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n",
    "    )(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "    ################################################################################################\n",
    "    # Here we will start with the upconvolution\n",
    "    ################################################################################################\n",
    "    up6 = Conv2D(\n",
    "        512, 2, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n",
    "    )(\n",
    "        # ____ insert the input of this first upsampling layer with using the function:\n",
    "        # << UpSampling2D()>> with the same size as the MaxPooling layers and as input the according layer.\n",
    "    )\n",
    "    # Because we now have the output of our UpConvolution but also we need the spatial information from the\n",
    "    # convolutional layers and MaxPooling we have to merge these two inputs with the function <<concatenate>>\n",
    "    # Figurativly speaking merge the gray and green arrows from Fig. 2\n",
    "    merge6 = concatenate([___, __], axis=3)\n",
    "    conv6 = Conv2D(\n",
    "        512, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n",
    "    )(merge6)\n",
    "    conv6 = Conv2D(\n",
    "        512, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n",
    "    )(conv6)\n",
    "\n",
    "    up7 = Conv2D(\n",
    "        256, 2, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n",
    "    )(UpSampling2D(size=(2, 2))(conv6))\n",
    "    merge7 = concatenate([conv3, up7], axis=3)\n",
    "    conv7 = Conv2D(\n",
    "        256, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n",
    "    )(merge7)\n",
    "    conv7 = Conv2D(\n",
    "        256, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n",
    "    )(conv7)\n",
    "\n",
    "    up8 = Conv2D(\n",
    "        128, 2, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n",
    "    )(UpSampling2D(size=(2, 2))(conv7))\n",
    "    merge8 = concatenate([conv2, up8], axis=3)\n",
    "    conv8 = Conv2D(\n",
    "        128, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n",
    "    )(merge8)\n",
    "    conv8 = Conv2D(\n",
    "        128, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n",
    "    )(conv8)\n",
    "\n",
    "    up9 = Conv2D(\n",
    "        64, 2, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n",
    "    )(UpSampling2D(size=(2, 2))(conv8))\n",
    "    merge9 = concatenate([conv1, up9], axis=3)\n",
    "    conv9 = Conv2D(\n",
    "        64, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n",
    "    )(merge9)\n",
    "    conv9 = Conv2D(\n",
    "        64, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n",
    "    )(conv9)\n",
    "    conv9 = Conv2D(\n",
    "        2, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n",
    "    )(conv9)\n",
    "\n",
    "    # In the last layer we chose the <sigmoid> as activation function because the output has to be between zero and one.\n",
    "    # We normalized out pixle values by deviding through 255.\n",
    "    # For our example we have only one class and thus only needing\n",
    "    # zero being white (our predicted object-segment) and 1 beeing black.\n",
    "    # but if we have more that one class the different colors will also lie inbetween 0 and 1 and every\n",
    "    # prediction is labeld with the appropriate number.\n",
    "\n",
    "    # Choose a function for your last layer which fullfill the requirements for classification at its best\n",
    "    conv10 = Conv2D(1, 1, activation=__)(conv9)\n",
    "\n",
    "    # With the following method <Model> we define out input and output of or Neural Network\n",
    "    # please complete:\n",
    "    model = Model(inputs=inputs, outputs=conv10)\n",
    "\n",
    "    # with Model.compile we configure and prepare our network for training. Thus we define our optimizer for\n",
    "    # learning with it's learning rate, the loss function and the metrics.\n",
    "    # Choose a loss you think fits best for this kind of task and also do so for the optimizer\n",
    "    # if your own coded optimizer and loss should be used you have to implement them accoridingly\n",
    "    model.compile(optimizer=__(learning_rate=__), loss=___, metrics=[\"accuracy\"])\n",
    "\n",
    "    # model.summary()\n",
    "\n",
    "    # this part we need for resume to a training or printing test, results for an already trained network, be careful\n",
    "    # that the input_size you defined for the model match with the one you defined for the model you take the pretrained\n",
    "    # weights from!\n",
    "    # Here you can play with some provided networks for more difficult tasks and do not train them expensivly on your own\n",
    "    # Computer\n",
    "    if pretrained_weights:\n",
    "        model.load_weights(pretrained_weights)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data\n",
    "\n",
    "So far we build the architecture of our neural net and the data, consists out of images and labels. To train the neural network in the next step the data has to be prepared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjusting and Augmenting the Data\n",
    "\n",
    "Before we feed the data to our neural network, it has to be adjusted properly. \n",
    "The first step always helps is to normalize our input data. Important is to normalize both the labels and the images, so that the neural network does not learn something wrong, warped or even nothing at all.\n",
    "In this tutorial we got the binary case and therefor the images and masks just need to be normalized.\n",
    "This is done by dividing through 255. \n",
    "The way we augment data in modern tensorflow is to use a Sequential model with augmentation layers, that return the input tensor in an augmented way. By using these layers we never need to convert our input tensors we created into a different format and can just later on run the input tensors and the labeled output tensors through the same model and get our augmented pair. \n",
    "It is very important to use the same seed in both `` adjust_training_images `` and `` adjust_training_masks ``, because if we for example rotate an image, the corresponding laber has to be rotated in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "##########################################################\n",
    "#           In this box some code must be added          #\n",
    "##########################################################\n",
    "\n",
    "def adjust_training_images(\n",
    "    rotation_range: float,\n",
    "    width_shift_range: float,\n",
    "    height_shift_range: float,\n",
    "    zoom_range: float,\n",
    "    resize_height: int,\n",
    "    resize_width: int,\n",
    "    flip=\"horizontal\",\n",
    "    interpolation=\"nearest\",\n",
    "    seed=1,\n",
    "):\n",
    "\n",
    "    sequential_augmentation = Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Rescaling(scale= __ / ___ ),\n",
    "            tf.keras.layers.RandomRotation(\n",
    "                factor=rotation_range, interpolation=interpolation, seed=seed\n",
    "            ),\n",
    "            tf.keras.layers.RandomZoom(\n",
    "                height_factor=zoom_range,\n",
    "                width_factor=zoom_range,\n",
    "                fill_mode=\"reflect\",\n",
    "                interpolation=interpolation,\n",
    "                seed=seed,\n",
    "            ),\n",
    "            tf.keras.layers.RandomFlip(mode=flip, seed=seed),\n",
    "            tf.keras.layers.RandomHeight(\n",
    "                factor=height_shift_range, interpolation=interpolation, seed=seed\n",
    "            ),\n",
    "            tf.keras.layers.RandomWidth(\n",
    "                factor=width_shift_range, interpolation=interpolation, seed=seed\n",
    "            ),\n",
    "            tf.keras.layers.Resizing(\n",
    "                resize_height,\n",
    "                resize_width,\n",
    "                interpolation=\"bilinear\",\n",
    "                crop_to_aspect_ratio=False,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return sequential_augmentation\n",
    "\n",
    "\n",
    "def adjust_training_masks(\n",
    "    rotation_range: float,\n",
    "    width_shift_range: float,\n",
    "    height_shift_range: float,\n",
    "    zoom_range: float,\n",
    "    resize_height: int,\n",
    "    resize_width: int,\n",
    "    flip=\"horizontal\",\n",
    "    interpolation=\"nearest\",\n",
    "    seed=1,\n",
    "):\n",
    "\n",
    "    sequential_augmentation = Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Rescaling(scale= __ / ___ ),\n",
    "            tf.keras.layers.RandomRotation(\n",
    "                factor=rotation_range, interpolation=interpolation, seed=seed\n",
    "            ),\n",
    "            tf.keras.layers.RandomZoom(\n",
    "                height_factor=zoom_range,\n",
    "                width_factor=zoom_range,\n",
    "                fill_mode=\"reflect\",\n",
    "                interpolation=interpolation,\n",
    "                seed=seed,\n",
    "            ),\n",
    "            tf.keras.layers.RandomFlip(mode=flip, seed=seed),\n",
    "            tf.keras.layers.RandomHeight(\n",
    "                factor=height_shift_range, interpolation=interpolation, seed=seed\n",
    "            ),\n",
    "            tf.keras.layers.RandomWidth(\n",
    "                factor=width_shift_range, interpolation=interpolation, seed=seed\n",
    "            ),\n",
    "            tf.keras.layers.Resizing(\n",
    "                resize_height,\n",
    "                resize_width,\n",
    "                interpolation=\"bilinear\",\n",
    "                crop_to_aspect_ratio=False,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return sequential_augmentation\n",
    "\n",
    "\n",
    "rescale_data = Sequential([tf.keras.layers.Rescaling(scale= __ / ___ )])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The training Generator\n",
    "\n",
    "The next function will be the ``trainGenerator``.\n",
    "Because deep learning task only get to learn something if they are trained with a lot of data, we have to provide it. But with the data we downloaded, there are only 30 images in the folder ``train/``. For a deep learning trask that is way to little. Therefor we will generating our own data with data augmentation. This function will use the data augmentation layers created above on the given images with layers like RandomRotation, RandomFlip and RandomZoom etc. Here the most important is that labels and images are transformed together with the same transformation functions. How good such an augmentation can replace more recordings strongly depends from the data on which we will learn. But for the given task the data augmentation works really good.\n",
    "The function used here to generate the Data is the image_dataset_from_directory provided within Keras. The documentations for more information can be found here \n",
    "https://keras.io/preprocessing/image/.\n",
    "\n",
    "To use the image augmentation function from above, we map the output onto the augmentation function. \n",
    "\n",
    "Why is it do important that you will chose the same seed for the mask and the image itself and what does seed stand for?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "##########################################################\n",
    "#           In this box some code must be added          #\n",
    "##########################################################\n",
    "def trainGenerator(\n",
    "    batch_size,\n",
    "    train_path,\n",
    "    image_folder,\n",
    "    mask_folder,\n",
    "    aug_dict,\n",
    "    image_color_mode=\"grayscale\",\n",
    "    mask_color_mode=\"grayscale\",\n",
    "    target_size=(256, 256),\n",
    "    seed=1,\n",
    "):\n",
    "    \"\"\"\n",
    "    can generate image and mask at the same time\n",
    "    use the same seed for image_datagen and mask_datagen to ensure the transformation for image and mask is the same\n",
    "    if you want to visualize the results of generator, set save_to_dir = \"your path\"\n",
    "    \"\"\"\n",
    "    # create two augmentation sequences with the same seed, to augment the images and the labels in the same way.\n",
    "    # This is crucial because otherwise the labels wouldn't fit the images anymore.\n",
    "    image_augmentation = adjust_training_images(**aug_dict)\n",
    "    mask_augmentation = adjust_training_masks(**aug_dict)\n",
    "\n",
    "    # Here we create a generator object and the map the augmentation and rescaling Sequences on the generators.\n",
    "    image_generator = (\n",
    "        image_dataset_from_directory(\n",
    "            train_path + \"/\" + image_folder,\n",
    "            labels=None,\n",
    "            label_mode=\"binary\",\n",
    "            color_mode=image_color_mode,\n",
    "            image_size=target_size,\n",
    "            batch_size=batch_size,\n",
    "            seed=seed,\n",
    "        )\n",
    "        .repeat()\n",
    "        .map(lambda x: ___ (x)) # What function do we want to map here?\n",
    "    )\n",
    "\n",
    "    mask_generator = (\n",
    "        image_dataset_from_directory(\n",
    "            train_path + \"/\" + mask_folder,\n",
    "            labels=None,\n",
    "            label_mode=\"binary\",\n",
    "            color_mode=mask_color_mode,\n",
    "            image_size=target_size,\n",
    "            batch_size=batch_size,\n",
    "            seed=seed,\n",
    "        )\n",
    "        .repeat()\n",
    "        .map(lambda x: ___ (x)) # What function do we want to map here?\n",
    "    )\n",
    "\n",
    "    train_generator = zip(image_generator, mask_generator)\n",
    "    for (img, mask) in train_generator:\n",
    "        yield (img, mask)\n",
    "\n",
    "\n",
    "\n",
    "# to understand what yield is: https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Testing Generator\n",
    "\n",
    "The testGenerator function will generate your test data on the images provided within the test folder. \n",
    "It will first read the images from the test-folder as gray images normalize them and resize them like defined in with the parameter target_size. Depending on the amount of object-classes you need the image vector will be extended. For our purpose again it is not necessary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def testGenerator(test_path, image_color_mode=\"grayscale\"):\n",
    "\n",
    "    image_generator = image_dataset_from_directory(\n",
    "        test_path,\n",
    "        labels=None,\n",
    "        label_mode=\"binary\",\n",
    "        color_mode=image_color_mode,\n",
    "        image_size=target_size,\n",
    "        batch_size=2,\n",
    "        shuffle=False,\n",
    "    ).map(lambda x: (rescale_data(x)))\n",
    "\n",
    "    return image_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The visualizer of the labels and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def labelVisualize(num_class, img):\n",
    "    img = img[:, :, 0] if len(img.shape) == 3 else img\n",
    "    img_out = np.zeros(img.shape + (3,))\n",
    "    for i in range(num_class):\n",
    "        img_out[img == i, :] = color_dict[i]\n",
    "    return img_out / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def saveResult(save_path, npyfile, num_class=2):\n",
    "    for i, item in enumerate(npyfile):\n",
    "        img = np.array(item[:, :, 0] * 255, dtype=np.uint8)\n",
    "        io.imsave(os.path.join(save_path, \"%d_predict.png\" % i), img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all functions we need to run our Unet are defined and thus we can start the training.\n",
    "\n",
    "First we need to augmentate our dataset which contains up to now only 30 images.\n",
    "Therefor we can define some parameters within out data augmentation dictionary \"data_gen_args\" for this augmentation. To get some results and not just a gray image as prediciton, you should start with small parameters here.\n",
    "\n",
    "Then we will generate the data with the beforehand defined parameters and the images given in the training folder. \n",
    "The first number hereby defines the batch_size, which you can change to a bigger number if your Computer fill in the requirements (GPU, Memory).\n",
    "\n",
    "Now it is time to built our UNet with just calling the function and than start the traing.\n",
    "You can change the \"steps_per_epoch\" and \"epochs\". The more you chose the better the results will get but also the longer the training will take.\n",
    "The Checkpoint-file will be saved within your Jupter Notebook folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "##########################################################\n",
    "#           In this box some code must be added          #\n",
    "##########################################################\n",
    "# if it fails to run the training on your tensorflow-gpu\n",
    "# rerun it on a tensorflow cpu version\n",
    "\n",
    "data_gen_args = dict(\n",
    "    rotation_range=__,\n",
    "    width_shift_range=__,\n",
    "    height_shift_range=__,\n",
    "    zoom_range=__,\n",
    "    resize_height=256,\n",
    "    resize_width=256,\n",
    "    flip=__,\n",
    "    interpolation=\"nearest\",\n",
    ")\n",
    "\n",
    "myGene = trainGenerator(2, \"data/membrane/train\", \"image\", \"label\", data_gen_args)\n",
    "\n",
    "model = unet()\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    \"unet_membrane.hdf5\", monitor=\"loss\", verbose=1, save_best_only=True\n",
    ")\n",
    "model.fit(myGene, steps_per_epoch=300, epochs=1, callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your Model\n",
    "\n",
    "When the training is done you can generate some prediction on your test data and save them within the test-image folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "testGene = testGenerator(\"data/membrane/test\")\n",
    "results = model.predict(testGene, 30, verbose=1)\n",
    "\n",
    "if not os.path.exists(\"data/prediction\"):\n",
    "    os.mkdir(\"data/prediction\")\n",
    "\n",
    "saveResult(\"data/prediction\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the results\n",
    "\n",
    "We can now either look in the folder how well our network was trained or just show some images and their predicitons here:\n",
    "If your prediction is just a gray image, your model wasn't trained good enough, and you might want to change some parameters (first of all steps_per_epoch and epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "test_path = \"data/membrane/test/\"\n",
    "predict_path = \"data/prediction/\"\n",
    "# number 0 to 29 your want to show\n",
    "img_number = 1\n",
    "display(Image(test_path + str(img_number) + \".png\"))\n",
    "display(Image(predict_path + str(img_number) + \"_predict.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next\n",
    "\n",
    "Some ideas of what you can do next\n",
    "\n",
    "- Try on cars \n",
    "- try on dataset with multiple labels \n",
    "- try on zebrafish repository (https://osf.io/snb6p/)\n",
    "- adapt architecture (loss, learning-rate,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources:<br>\n",
    "[Fig2 - https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png)<br>\n",
    "[Fig3 - https://arxiv.org/pdf/1505.04597.pdf](https://arxiv.org/pdf/1505.04597.pdf) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
